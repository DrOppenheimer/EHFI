otudf = data.frame(prev = samobs, sums = taxa_sums(template))
otudf = otudf[order(-otudf$prev, -otudf$sums), ]
# Trim all but the first nOTUs
template = prune_taxa(rownames(otudf)[1:nOTUs], template)
template
template1 = subset_samples(template, SampleType == sampletypes[1])
template2 = subset_samples(template, SampleType == sampletypes[2])
# Forced mixing.
# Define function for mixing
mix_microbes = function(template1, template2, unmixfac) {
require("phyloseq")
# Check that the number of taxa are equal
if (!identical(ntaxa(template1), ntaxa(template2))) {
stop("number of OTUs b/w template1 and template2 must be equal")
}
# Expects templates to be a 1-sample dataset.
if (!identical(nsamples(template1), 1L)) {
stop("`template1` should have only 1 sample")
}
if (!identical(nsamples(template2), 1L)) {
stop("`template2` should have only 1 sample")
}
# Enforce taxa_are_rows to FALSE
if (taxa_are_rows(template1)) {
template1 <- t(template1)
}
if (taxa_are_rows(template2)) {
template2 <- t(template2)
}
# Define a vector version of each template for subsampling
x1 = as(otu_table(template1), "numeric")
x2 = as(otu_table(template2), "numeric")
# Create 'dirty' multinomial.  Defined artificial mixing.  Create mixed
# multinomial by adding counts from the other in precise proportion, a total
# of Library Size / Effect Size
addToTemplate1 = round((sum(x1) * x2/(sum(x2) * unmixfac)), 0)
# Add them together to create 'dirty' multinomial This adds the fractional
# subsampled abundances to the template
mat1 = matrix((addToTemplate1 + x1), nrow = 1)
rownames(mat1) <- sample_names(template1)
colnames(mat1) <- taxa_names(template1)
otu_table(template1) <- otu_table(mat1, taxa_are_rows = FALSE)
return(template1)
}
# merge the template components together into one sample.
template1 = merge_samples(template1, "SampleType")
template2 = merge_samples(template2, "SampleType")
# Simulation Function
## Input
##     postfix, template, J, n (number or reads)
## Output
##     phyloseq object, incorporating simulation results and inputs
microbesim = function(postfix = "sim", template, templatex, unmixfac, J, n = 10000) {
# Generate `J` simulated microbiomes with `n` total reads each (all the
# same, or n has length equal to the value of `J`), with subsamples drawn
# from `template`.  `postfix` is a dummy idenitifer added to help
# distinguish simulated samples in downstream code.
require("phyloseq")
# Perform the mixing here, so each replicate is a separate random mix
template = mix_microbes(template, templatex, unmixfac)
# call the proporitions vector `pi`, similar to nomenclature from DMN
pi = taxa_sums(template)
# n must be a scalar (recycled as the number of reads for every simulation)
# or it can be vector of length equal to J, the number of samples being
# simulated.
if (length(J) != 1) {
stop("Length of J should be 1.")
}
if (length(n) != 1 & length(n) != J) {
stop("n should be length 1, or length J.")
}
# Actually create the simulated abundance table
simat = mapply(function(i, x, sample.size) {
if (FALSE)
{
print(i)
}  # i is a dummy iterator
phyloseq:::rarefaction_subsample(x, sample.size)
}, i = 1:J, sample.size = n, MoreArgs = list(x = pi), SIMPLIFY = TRUE)
simat = t(simat)
# Add the OTU names to the OTU (column) indices
colnames(simat) <- names(pi)
# Add new simulated sample_names to the row (sample) indices
rownames(simat) <- paste(i, "::", 1:nrow(simat), postfix, sep = "")
# Put simulated abundances together with metadata as a phyloseq object
OTU = otu_table(simat, taxa_are_rows = FALSE)
# Define data.frame that will become sample_data
SDF = data.frame(sample = sample_names(OTU), TableNumber = i, type = "simulated",
stringsAsFactors = FALSE)
SDF$postfix <- postfix
rownames(SDF) <- sample_names(OTU)
SD = sample_data(SDF)
# Return a phyloseq object
return(phyloseq(OTU, SD))
}
# Define a function for sampling from the library sizes as a way of including the “noise” derived from this aspect of the data as well.
# rescale the sum of reads in the original raw(-ish) template data to the
# expected library size being requested here
sumsim = function(n, sumtemplate, J) {
# `n` - expected size target `sumtemplate` - the template vector of library
# sizes observed in template `J` - The number of sample sizes to return
# sumtemplate = sampsums n = 2000 J = 103
scaledSums = round(n * (sumtemplate/median(sumtemplate)))
return(sample(scaledSums, size = J, replace = TRUE))
}
cl <- makeCluster(Ncores)
registerDoParallel(cl)
deseq_varstab = function(physeq, sampleConditions = rep("A", nsamples(physeq)),
...) {
require("DESeq")
# Enforce orientation.
if (!taxa_are_rows(physeq)) {
physeq <- t(physeq)
}
x = as(otu_table(physeq), "matrix")
# The same tweak as for edgeR to avoid NaN problems that cause the workflow
# to stall/crash.
x = x + 1
# Create annotated data.frame with the taxonomy table, in case it is useful
# later
taxADF = as(data.frame(as(tax_table(physeq), "matrix"), stringsAsFactors = FALSE),
"AnnotatedDataFrame")
cds = newCountDataSet(x, sampleConditions, featureData = taxADF)
# First estimate library size factors
cds = estimateSizeFactors(cds)
# Variance estimation, passing along additional options
cds = estimateDispersions(cds, ...)
# Determine which column(s) have the dispersion estimates
dispcol = grep("disp\\_", colnames(fData(cds)))
# Enforce that there are no infinite values in the dispersion estimates
if (any(!is.finite(fData(cds)[, dispcol]))) {
fData(cds)[which(!is.finite(fData(cds)[, dispcol])), dispcol] <- 0
}
vsmat = exprs(varianceStabilizingTransformation(cds))
otu_table(physeq) <- otu_table(vsmat, taxa_are_rows = TRUE)
return(physeq)
}
simpletrim = function(physeq, J) {
if (taxa_are_rows(physeq)) {
physeq = t(physeq)
}
# `prevalence` is the fraction of total samples in which an OTU is observed
# at least `minobs` times.
prevalence = apply(as(otu_table(physeq), "matrix"), 2, function(x, minobs) {
sum(x > minobs)
}, minobs)/(2 * J)
# Will only keep OTUs that appear in more than X% of samples
keepOTUs = prevalence > 0.05
# Keep only those OTUs with total reads greater than 0.5x the number of
# samples.
keepOTUs = keepOTUs & taxa_sums(physeq) > (0.5 * J)
return(prune_taxa(keepOTUs, physeq))
}
# Initialize the simulation lists simlist0 and simlist
simlist0 = simlistuntrim = simlist
# checks apply(as(otu_table(simlistuntrim[[1]]), 'matrix'), 2, function(x,
# tot){sum(x>tot)}, minobs)/(2*J) sapply(lapply(simlistuntrim, taxa_sums),
# function(x, J){sum(x < (0.5*J))}, J)/nOTUs Trim simlist0
simlist_0 = simlistuntrim = simlist
names(simlist) <- simparams
simlist <- foreach(i = simparams, .packages = c("phyloseq")) %dopar% {
# i = simparams[4] Initialize
n = sim = sim1 = sim2 = n1 = n2 = NULL
# cat(i, '\n')
n = as.numeric(strsplit(i, comdelim)[[1]][1])
mixfac = as.numeric(strsplit(i, comdelim)[[1]][3])
# Rarely a simulation has a weird value and fails.  Catch these with `try`,
# and repeat the simulation call if error (it will be a new seed)
tryAgain = TRUE
infiniteloopcounter = 1
while (tryAgain & infiniteloopcounter < 5) {
n1 = sumsim(n, sampsums, J)
n2 = sumsim(n, sampsums, J)
sim1 = microbesim(sampletypes[1], template1, template2, mixfac, J, n1)
sim2 = microbesim(sampletypes[2], template2, template1, mixfac, J, n2)
if (is.null(sim1) | is.null(sim2) | is.null(n1) | is.null(n2) | inherits(sim1,
"try-error") | inherits(sim2, "try-error")) {
tryAgain = TRUE
infiniteloopcounter = infiniteloopcounter + 1
} else {
tryAgain = FALSE
}
}
if (infiniteloopcounter >= 5) {
stop("Consistent error found during simulation. Need to investigate cause.")
}
# Merge the two simulated datasets together into one phyloseq object and add
# back tree.
sim = merge_phyloseq(sim1, sim2)
sim = merge_phyloseq(sim, tax_table(GlobalPatterns), phy_tree(GlobalPatterns))
return(sim)
}
names(simlist) <- simparams
deseq_varstab = function(physeq, sampleConditions = rep("A", nsamples(physeq)),
...) {
require("DESeq")
# Enforce orientation.
if (!taxa_are_rows(physeq)) {
physeq <- t(physeq)
}
x = as(otu_table(physeq), "matrix")
# The same tweak as for edgeR to avoid NaN problems that cause the workflow
# to stall/crash.
x = x + 1
# Create annotated data.frame with the taxonomy table, in case it is useful
# later
taxADF = as(data.frame(as(tax_table(physeq), "matrix"), stringsAsFactors = FALSE),
"AnnotatedDataFrame")
cds = newCountDataSet(x, sampleConditions, featureData = taxADF)
# First estimate library size factors
cds = estimateSizeFactors(cds)
# Variance estimation, passing along additional options
cds = estimateDispersions(cds, ...)
# Determine which column(s) have the dispersion estimates
dispcol = grep("disp\\_", colnames(fData(cds)))
# Enforce that there are no infinite values in the dispersion estimates
if (any(!is.finite(fData(cds)[, dispcol]))) {
fData(cds)[which(!is.finite(fData(cds)[, dispcol])), dispcol] <- 0
}
vsmat = exprs(varianceStabilizingTransformation(cds))
otu_table(physeq) <- otu_table(vsmat, taxa_are_rows = TRUE)
return(physeq)
}
# Perform normalizations
simpletrim = function(physeq, J) {
if (taxa_are_rows(physeq)) {
physeq = t(physeq)
}
# `prevalence` is the fraction of total samples in which an OTU is observed
# at least `minobs` times.
prevalence = apply(as(otu_table(physeq), "matrix"), 2, function(x, minobs) {
sum(x > minobs)
}, minobs)/(2 * J)
# Will only keep OTUs that appear in more than X% of samples
keepOTUs = prevalence > 0.05
# Keep only those OTUs with total reads greater than 0.5x the number of
# samples.
keepOTUs = keepOTUs & taxa_sums(physeq) > (0.5 * J)
return(prune_taxa(keepOTUs, physeq))
}
# Perform initializations, and trimming
# Initialize the simulation lists simlist0 and simlist
simlist0 = simlistuntrim = simlist
# checks apply(as(otu_table(simlistuntrim[[1]]), 'matrix'), 2, function(x,
# tot){sum(x>tot)}, minobs)/(2*J) sapply(lapply(simlistuntrim, taxa_sums),
# function(x, J){sum(x < (0.5*J))}, J)/nOTUs Trim simlist0
simlist0 = lapply(simlistuntrim, simpletrim, J)
# Now perform the normalizations.
# Replace simlist with empty list. Will add different norm types to it.
simlist = vector("list", 0)
simlist$none <- simlist0
# ...
ls()
library(knitr)
library(markdown)
install.packages("knitr")
install.packages("knitr")
test1="test1"
test2="test2"
if( identical(test1,"test1") && identical(test2,"test2")){print("poop")}
if( identical(test1,"test1") && identical(test2,"test")){print("poop")}
print("\"")
print("/"")
)
print("/"")g
print(""")
)
)
))))
print("test")
print("\test")
print("\"test")
print("/"test")
print(""test""")
print(""test"")
?print
print(\\")
print(\\""")
print(\\\""")
print(\\\")
print(\\\)
print(/)
print(//)
print("//")
print("You must supply metadata for any DESeq method other than "blind"")
print("\"')
print("\"")
print("\"")
paste(print("You must supply metadata for any DESeq method other than, "\"", blind,"\""""))
paste(print("You must supply metadata for any DESeq method other than, "\"", blind,"\"""))
paste(print("You must supply metadata for any DESeq method other than, "\"", blind,"\""))
print(paste("You must supply metadata for any DESeq method other than, "\"", blind,"\""))
print(paste("You must supply metadata for any DESeq method other than, "\"", blind,"\""""))
print(paste("You must supply metadata for any DESeq method other than, """, blind,""""))
print(paste("You must supply metadata for any DESeq method other than, """, blind,""""""))
print(paste("You must supply metadata for any DESeq method other than, """, blind))
print(paste("You must supply metadata for any DESeq method other than, "\"", blind))
print(paste("You must supply metadata for any DESeq method other than, "/"", blind))
print(cat("You must supply metadata for any DESeq method other than, "/"", blind))
print(cat("You must supply metadata for any DESeq method other than, "\"", blind))
print(cat("You must supply metadata for any DESeq method other than, "\", blind))
print(cat("You must supply metadata for any DESeq method other than, "\", blind"))
print(cat("You must supply metadata for any DESeq method other than, "\"", blind"))
print(cat("You must supply metadata for any DESeq method other than, '"', blind'"'"))
print(cat("You must supply metadata for any DESeq method other than", '"', blind'"'"))
print(cat("You must supply metadata for any DESeq method other than", '"', "blind",'"'))
print(paste("You must supply metadata for any DESeq method other than", '"', "blind",'"'))
print(paste("You must supply metadata for any DESeq method other than", ", "blind",'"'))
print(paste("You must supply metadata for any DESeq method other than", """, "blind",'"'))
print(paste("You must supply metadata for any DESeq method other than", "\"", "blind",'"'))
print(paste("You must supply metadata for any DESeq method other than", "/"", "blind",'"'))
test1
test2
test3="test3"
if( identical(test1,"test1" ) && ( identical(test2,"test2") && identical(test3,"test3")  )){print("poop")}
if( identical(test1,"test1" ) && ( identical(test2,"test2") && identical(test3,"test")  )){print("poop")}
if( identical(test1,"test1" ) && ( identical(test2,"test2") || identical(test3,"test")  )  ){print("poop")}
if( identical(test1,"test1" ) && ( identical(test2,"test") || identical(test3,"test")  )  ){print("poop")}
if( identical(test1,"test1" ) && ( identical(test2,"test") || identical(test3,"test3")  )  ){print("poop")}
library(DESeq)
?estimateDispersions
?heatmap.2
library(gplots)
?heatmap.2
library(matR)
remove.packages(matR)
remove.packages("matR")
library(matR)
install.packages("devtools")          # package to installR packages direct  from github
library(devtools)
install_github("MG-RAST/matR")  # installs the package from this repository
dependencies()                             # installs all matR dependencices in R
install.pacakges("RCurl", ""devtools)
install.packages("RCurl", ""devtools)
install.packages(c("RCurl", ""devtools))
install.packages(c("RCurl", "devtools"))
install.packages(c("RCurl", "devtools"))
install.packages("devtools")
install.packages("RCurl")
library(matR)
.libPaths()
?remove.pacakges
?remove.pacakges()
?remove.packages
remove.pacakges("matR", lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
remove.packages("matR", lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library(matR)
remove.packages("matR", lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library(matR)
remove.packages("RCurl", lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
remove.packages("devtools", lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library(devtools)
library(RCurl)
R.Version()
install.packages("RCurl")
install.packages("devtools")
library(RCurl)
library(devtools)
install_github("MG-RAST/matR")  # installs the package from this repository
remove.packages("devtools", lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
remove.packages("RCurl", lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
remove.packages(c("matR","devtools","RCurl"), lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
?install.packages
install.packages(c("RCurl", "devtools"), dependencies=TRUE)
install.packages(c("RCurl", "devtools"), dependencies = TRUE)
library(devtools)
remove.packages(c("matR","devtools","RCurl"), lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
library(devtools)
library(RCurl)
library(matR)
install.packages(c("RCurl", "devtools"), dependencies = TRUE))
install.packages(c("RCurl", "devtools"), dependencies = TRUE)
remove.packages(c("matR","devtools","RCurl"), lib="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
install.packages("devtools", dependencies = TRUE)
library(RCurl)
library(devtools)
install_github("MG-RAST/matR")
library(matR)
dependencies()
library(matR)
demo(package="matR")
library(matR)
data(data)
test <- collection(guts)
traceback()
msession$ursl()
msession$urls()
ls()
msession$server()
msession$servers
msession$servers()
msession$server()
msession$server()<- "http://api.metagenomics.anl.gov"
msession$server <- "http://api.metagenomics.anl.gov"
msession$server()
msession$server
test <- collection(guts)
msession$urls()
msession$server (msession$servers()$prod)
msession$server( msession$servers()$prod )
msession
msession()
msession
msession$server
msession$server <- servers[1]
msession$server
test <- collection(guts)
msession$server()
msession$server()
msession$server("http://api.metagenomics.anl.gov")
traceback()
test <- collection(guts)
traceback()
msession$servers()$prod
msession$server = msession$servers()$prod
install_github("MG-RAST/matR", ref="kevin")
library(devtools)
install_github("MG-RAST/matR", ref="kevin")  # i.e. ref is the branchname, default is master
install_github("MG-RAST/matR", ref="kevin")  # i.e. ref is the branchname, default is master
install_github("MG-RAST/matR", ref="kevin")  # i.e. ref is the branchname, default is master
data(data)
library(matR)
data(data)
install_github("MG-RAST/matR")
library(matR)
q()
library(devtools)
?install_github
test_data <- import_data("/Users/kevin/Documents/Projects/matR/demos/soil_WGS/6-30-14/soil_WGS_SS_L3_raw.6-30-14.txt")
test_data <- test_data[,1:5]
# perform rarefaction and create data objects for plotting
subset_sizes <- c(10,100,1000)
all.rar_results.summary <- numeric()
all.subset_sizes <- numeric()
my_stage.temp <- character()
for (i in 1:ncol(test_data)){
rar_results <- rarefy(test_data[i,], subset_sizes)
rar_results.summary <- numeric()
for (j in 1:length(my_sample)){
rar_results.summary <- c(rar_results.summary, mean(rar_results[,j]))
}
all.subset_sizes <- c(all.subset_sizes, subset_sizes)
#names(rar_results.summary)<- my_sample
all.rar_results.summary <- c(all.rar_results.summary, rar_results.summary)
my_stage.temp <- c(my_stage.temp, c(rep( colnames(test_data)[i], ncol(test_data))) )
#my_stage.temp <- factor( c(rep( colnames(test_data)[i], ncol(test_data))) )
}
my_stage <- factor(my_stage.temp)
my_data_frame <- data.frame( subset_sizes=all.subset_sizes, rar=all.rar_results.summary, sample=my_stage)
# pretty plot of the rarefaction
install.packages("ggplot2")
library(ggplot2)
my_plot <- ggplot( my_data_frame, aes(x=subset_sizes, y=rar, color=sample)) + ggtitle("Function rarefaction") + ylab("Number Species") + xlab("Subsample Size")
my_plot + geom_line()
test_data <- import_data("/Users/kevin/Documents/Projects/matR/demos/soil_WGS/6-30-14/soil_WGS_SS_L3_raw.6-30-14.txt")
subset_sizes <- c(10,100,1000)
all.rar_results.summary <- numeric()
all.subset_sizes <- numeric()
my_stage.temp <- character()
for (i in 1:ncol(test_data)){
rar_results <- rarefy(test_data[i,], subset_sizes)
rar_results.summary <- numeric()
for (j in 1:length(my_sample)){
rar_results.summary <- c(rar_results.summary, mean(rar_results[,j]))
}
all.subset_sizes <- c(all.subset_sizes, subset_sizes)
#names(rar_results.summary)<- my_sample
all.rar_results.summary <- c(all.rar_results.summary, rar_results.summary)
my_stage.temp <- c(my_stage.temp, c(rep( colnames(test_data)[i], ncol(test_data))) )
#my_stage.temp <- factor( c(rep( colnames(test_data)[i], ncol(test_data))) )
}
my_stage <- factor(my_stage.temp)
my_data_frame <- data.frame( subset_sizes=all.subset_sizes, rar=all.rar_results.summary, sample=my_stage)
# pretty plot of the rarefaction
install.packages("ggplot2")
library(ggplot2)
my_plot <- ggplot( my_data_frame, aes(x=subset_sizes, y=rar, color=sample)) + ggtitle("Function rarefaction") + ylab("Number Species") + xlab("Subsample Size")
my_plot + geom_line()
install.packages("ggplot2")
my_plot + geom_line()
my_plot + stat_smooth()
remove.packages("matR")
install.packages("matR")
library(matR0
library(matR)
vignette(matr)
vignette(matR)
demos(matR)
demos
demos()
demos(matR)
demo(matR)
vignetter(package="matR")
vignette(package="matR")
?metadata
demo(package='matR')
demo(package='matR')
step.through('simple')
searcch()
search()
install.packages("matR",depend=T)
install.packages("matR",dependencies=T)
?install.packages
packageDescription('matR')
?install.packages
q()
view.parameters()
view.parameters
setwd("/Users/kevin/git/AMETHST/AMETHST/datasets/AMETHST_data/Human_microbiome_data")
fix_lt("HMP.DESeq_normed_commands.txt")
source("~/git/matR-apps.DrOppenheimer/matR-apps/fix_lt.r")
fix_lt("HMP.DESeq_normed_commands.txt")
